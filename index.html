<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Jingyi Xiang</title> <meta name="author" content="Jingyi Xiang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jingyi-xiang.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research</a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/Xiang_Jingyi_CV.pdf">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/robots/">Robots</a> </li> <li class="nav-item "> <a class="nav-link" href="/art/">Art</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Jingyi Xiang</span> 向婧怡 <span style="font-size:18pt">(She/Her)</span> </h1> <p class="desc"><i>Pronounced as Jing-Yi She-aang</i></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>I am an undergraduate student studying Electrical Engineering at the University of Illinois Urbana-Champaign with research interests in perception, tactile sensing, and robotic manipulation.</p> <p>I will be joining the <a href="http://robotouchlab.web.illinois.edu/" rel="external nofollow noopener" target="_blank">RoboTouch Lab</a> at UIUC as a Computer Science Ph.D. student starting Fall 2024, advised by <a href="http://robotouch.ri.cmu.edu/yuanwz/" rel="external nofollow noopener" target="_blank">Prof. Wenzhen Yuan</a>. Previously, I worked with the <a href="http://bretl.csl.illinois.edu/" rel="external nofollow noopener" target="_blank">Bretl Research Group</a> under the mentorship of Ph.D. candidate <a href="https://hollydinkel.github.io/" rel="external nofollow noopener" target="_blank">Holly Dinkel</a> as an undergraduate researcher, where I led the development of tracking algorithms for deformable linear objects.</p> <p> </p> </div> <h2><a href="/publications/" style="color: inherit;">Publications</a></h2> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/squeezebot-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/squeezebot-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/squeezebot-1400.webp"></source> <img src="/assets/img/publication_preview/squeezebot.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="squeezebot.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="huang2023kitchen" class="col-sm-8"> <div class="title">Kitchen Artist: Precise Control of Liquid Dispensing for Gourmet Plating</div> <div class="author"> <a href="https://joehjhuang.github.io/" rel="external nofollow noopener" target="_blank">Hung-Jui Huang</a>, <em><strong>Jingyi Xiang</strong></em>, and <a href="http://robotouch.ri.cmu.edu/yuanwz/" rel="external nofollow noopener" target="_blank">Wenzhen Yuan</a> </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/pdf/2311.12185.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/hZlQyrqnMsI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">VIDEO</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> </div> <div class="abstract hidden"> <p>Manipulating liquid is widely required for many tasks, especially in cooking. A common way to address this is extruding viscous liquid from a squeeze bottle. In this work, our goal is to create a sauce plating robot, which requires precise control of the thickness of squeezed liquids on a surface. Different liquids demand different manipulation policies. We command the robot to tilt the container and monitor the liquid response using a force sensor to identify liquid properties. Based on the liquid properties, we predict the liquid behavior with fixed squeezing motions in a data-driven way and calculate the required drawing speed for the desired stroke size. This open-loop system works effectively even without sensor feedback. Our experiments demonstrate accurate stroke size control across different liquids and fill levels. We show that understanding liquid properties can facilitate effective liquid manipulation. More importantly, our dish garnishing robot has a wide range of applications and holds significant commercialization potential.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/trackdlo_preview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/trackdlo_preview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/trackdlo_preview-1400.webp"></source> <img src="/assets/img/publication_preview/trackdlo_preview.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="trackdlo_preview.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xiang2023trackdlo" class="col-sm-8"> <div class="title">TrackDLO: Tracking Deformable Linear Objects Under Occlusion with Motion Coherence</div> <div class="author"> <em><strong>Jingyi Xiang</strong></em>, <a href="https://hollydinkel.github.io/" rel="external nofollow noopener" target="_blank">Holly Dinkel</a>, Harry Zhao, Naixiang Gao, <a href="https://brian.coltin.org/" rel="external nofollow noopener" target="_blank">Brian Coltin</a>, <a href="https://longhorizon.org/trey/" rel="external nofollow noopener" target="_blank">Trey Smith</a>, and <a href="http://bretl.csl.illinois.edu/" rel="external nofollow noopener" target="_blank">Timothy Bretl</a> </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters</em>, Aug 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="/assets/pdf/trackdlo.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/RMDLO/trackdlo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://youtu.be/MxqNJsen5eg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">VIDEO</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/LRA.2023.3303710"></span> </div> <div class="abstract hidden"> <p>The TrackDLO algorithm estimates the shape of a Deformable Linear Object (DLO) under occlusion from a sequence of RGB-D images. TrackDLO is vision-only and runs in real-time. It requires no external state information from physics modeling, simulation, visual markers, or contact as input. The algorithm improves on previous approaches by addressing three common scenarios which cause tracking failure: tip occlusion, mid-section occlusion, and self-occlusion. This is achieved through the application of Motion Coherence Theory to impute the spatial velocity of occluded nodes, the use of the topological geodesic distance to track self-occluding DLOs, and the introduction of a non-Gaussian kernel that only penalizes lower-order spatial displacement derivatives to reflect DLO physics. Improved real-time DLO tracking under mid-section occlusion, tip occlusion, and self-occlusion is demonstrated experimentally. The source code and demonstration data are publicly released.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/multidlo-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/multidlo-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/multidlo-1400.webp"></source> <img src="/assets/img/publication_preview/multidlo.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="multidlo.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xiang2023multidlo" class="col-sm-8"> <div class="title">Simultaneous Shape Tracking of Multiple Deformable Linear Objects with Global-Local Topology Preservation</div> <div class="author"> <em><strong>Jingyi Xiang</strong></em>, and <a href="https://hollydinkel.github.io/" rel="external nofollow noopener" target="_blank">Holly Dinkel</a> </div> <div class="periodical"> <em>In Workshop on Representing and Manipulating Deformable Objects, IEEE International Conference on Robotics and Automation (ICRA)</em>, May 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/pdf/2310.13245.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/RMDLO/multi-dlo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/multi_dlo_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://youtu.be/hfiqwMxitqA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">VIDEO</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> </div> <div class="abstract hidden"> <p>This work presents an algorithm for tracking the shape of multiple entangling Deformable Linear Objects (DLOs) from a sequence of RGB-D images. This algorithm runs in real-time and improves on previous single-DLO tracking approaches by enabling tracking of multiple objects. This is achieved using Global-Local Topology Preservation (GLTP). This work uses the geodesic distance in GLTP to define the distance between separate objects and the distance between different parts of the same object. Tracking multiple entangling DLOs is demonstrated experimentally. The source code is publicly released. </p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/wire_segmentation-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/wire_segmentation-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/wire_segmentation-1400.webp"></source> <img src="/assets/img/publication_preview/wire_segmentation.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="wire_segmentation.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="dinkel2022RMDLO" class="col-sm-8"> <div class="title">Wire Point Cloud Instance Segmentation from RGBD Imagery with Mask R-CNN</div> <div class="author"> <a href="https://hollydinkel.github.io/" rel="external nofollow noopener" target="_blank">Holly Dinkel*</a>, <em><strong>Jingyi Xiang*</strong></em>, Harry Zhao, <a href="https://brian.coltin.org/" rel="external nofollow noopener" target="_blank">Brian Coltin</a>, <a href="https://longhorizon.org/trey/" rel="external nofollow noopener" target="_blank">Trey Smith</a>, and <a href="http://bretl.csl.illinois.edu/" rel="external nofollow noopener" target="_blank">Timothy Bretl</a> </div> <div class="periodical"> <em>In Workshop on Representing and Manipulating Deformable Objects, IEEE International Conference on Robotics and Automation (ICRA)</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://deformable-workshop.github.io/icra2022/spotlight/WDOICRA2022_08.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/eqgZQckCDOY" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">VIDEO</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> </div> <div class="abstract hidden"> <p>Perception of the shapes of deforming objects like wires enables their monitoring and manipulation by autonomous robots. This paper presents detection, classification, and instance segmentation of deformable wires from a cluttered scene in RGBD imagery. This work uses the Detectron2 implementation of Mask R-CNN trained with the PointRend mask head on the UIUCWires dataset as the framework for wire instance segmentation on RGB imagery, a method demonstrated to perform well for the instance segmentation task in previous work. In this work, the instance bitmask is directly used to segment individual object point clouds, an important step toward wire shape representation for manipulation tasks.</p> </div> </div> </div> </li></ol> </div> * Equal Contribution <div class="social"> <div class="contact-icons"> <a href="mailto:%6A%69%6E%67%79%69%78%34@%69%6C%6C%69%6E%6F%69%73.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0003-0727-3098" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=ql1kzlkAAAAJ&amp;hl=en&amp;oi=ao" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/jingyi-xiang" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/jingyi-xiang" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Jingyi Xiang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>